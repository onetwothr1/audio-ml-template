# wandb
name:
version:

seed_everything: 42

model:
  class_path: BaseLine
  init_args:
    net:
      class_path: Cifar10Resnet18
      init_args:
        num_classes: 10
    loss_module:
      class_path: CrossEntropyLoss

trainer:
  accelerator: gpu
  precision: 16-mixed
  n_epoch: 3
  check_val_every_n_epoch: 1
  plugins:
    - AsyncCheckpointIO

data:
  class_path:
  init_args:
    root: data
    batch_size: 128
    val_split: 0.2
    transforms:
      class_path: 

optimizer:
  class_path: SGD
  init_args:
    lr: 0.2
    momentum: 0.9
    weight_decay: 0.0001

lr_scheduler:
  class_path: CosineAnnealingLR

mel_spectrogram:
  sample_rate: 48000
  n_fft: 2048 # n_fft >= win_length. use value of 2^n for computation speed
  win_length: 1200 # default 25ms in linguistics field 
  hop_length: 480 # default 10ms
  n_mel_filter: 40
  pad: 50
  f_min: 25
  f_max: 7500